{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRBhMZtdf94"
      },
      "source": [
        "# Download dataset with Pytorch\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNE5Qqw_rdsV"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKAZ-aQtrg8q",
        "outputId": "69cbeb14-71db-4f3e-f444-117f78e68ad7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torchvision import models, datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# Define transformation for each image\n",
        "transform  = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: np.array(x).flatten()) #Stretch image into row [32,32,3] -> [3072]\n",
        "])\n",
        "\n",
        "# Download a CIFAR10 dataset\n",
        "dataset = datasets.CIFAR10(\"content\",\n",
        "                           train=True,\n",
        "                           transform = transform,\n",
        "                           download=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3F4IoDErlcw"
      },
      "source": [
        "## Split dataset & define dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "id": "Fu02DmABYxoh",
        "outputId": "cf2c3d0b-81d8-4130-d3b6-24a6ab36ec89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([256, 3072]) torch.Size([256])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_ds, val_ds, _= random_split(dataset, [20000,1000 ,29000])\n",
        "# Hint: Perform debug on smaller subset\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size = batch_size)\n",
        "val_loader = DataLoader(val_ds, batch_size = batch_size)\n",
        "\n",
        "# Test Dataloader\n",
        "for images, class_nums in train_loader:\n",
        "  print(images.shape,class_nums.shape) # class_nums are tensor!\n",
        "  pil_img = Image.fromarray(images[0].reshape(32,32,3).numpy())\n",
        "  display(pil_img,\n",
        "          class_nums[0].item()\n",
        "          )\n",
        "  break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqC6xXhSeUPr"
      },
      "source": [
        "# Implement LinearClassifier class for CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "vaxIJUM7eQUp"
      },
      "outputs": [],
      "source": [
        "class LinearClassifier:\n",
        "  def __init__(self, labels, cross_entropy_loss = False, reg = 0.1):\n",
        "    self.labels = labels\n",
        "    self.classes_num = len(labels)\n",
        "    # Generate a random weight matrix of small numbers\n",
        "    # You can change this code\n",
        "    self.W = np.random.randn(3072 + 1, self.classes_num) * 0.0001\n",
        "    self.batch_size = 200\n",
        "    self.cross_entropy_loss = cross_entropy_loss\n",
        "    self.reg = reg\n",
        "\n",
        "\n",
        "  def train(self, x_batch, y_batch, learning_rate = 1e-8):\n",
        "    \"\"\"\n",
        "      Arguments:\n",
        "        x  (numpy.array): collection of objects (batch)\n",
        "        y  (numpy.array): collection of integer\n",
        "        representing a class number for objects from x\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    x_batch = self.add_bias(x_batch)\n",
        "    loss, grad = self.loss(x_batch, y_batch)\n",
        "\n",
        "    # Update weights (self.W)\n",
        "\n",
        "    self.W -= learning_rate * grad\n",
        "\n",
        "    return loss\n",
        "  \n",
        "  def add_bias(self, x):\n",
        "    return np.concatenate([np.ones((x.shape[0], 1)), x], axis=1)\n",
        "\n",
        "  def loss(self,x, y): # x and y are batches\n",
        "    \"\"\"\n",
        "      Arguments:\n",
        "        x  (numpy.array): collection of objects (batch)\n",
        "        y  (numpy.array): collection of integer\n",
        "        representing a class number for objects from x\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    loss = 0.0\n",
        "    dW = np.zeros(self.W.shape)\n",
        "\n",
        "    # Calculate Multiclass SVM or Cross-entropy loss over a batch\n",
        "\n",
        "    # Calculate gradients (dL/dW) and store it in dW\n",
        "\n",
        "    # Hinge loss\n",
        "    if not self.cross_entropy_loss:\n",
        "      for i in range(x.shape[0]):\n",
        "        scores = x[i].dot(self.W)\n",
        "        score_i = scores[y[i]]\n",
        "\n",
        "        n_positive_margin = 0\n",
        "        for j in range(self.classes_num):\n",
        "          if j == y[i]:\n",
        "            continue\n",
        "          \n",
        "          margin = scores[j] - score_i + 1\n",
        "          if margin > 0:\n",
        "            loss += margin\n",
        "            dW[:, j] += x[i]\n",
        "            n_positive_margin += 1\n",
        "        dW[:, y[i]] -= n_positive_margin * x[i]\n",
        "\n",
        "      loss /= x.shape[0]\n",
        "      dW /= x.shape[0]\n",
        "\n",
        "    # Cross-entropy loss\n",
        "    else:\n",
        "      for i in range(x.shape[0]):\n",
        "        scores = x[i].dot(self.W)\n",
        "        escores = np.exp(scores)\n",
        "        q = escores / np.sum(escores)\n",
        "        loss += -scores[y[i]] + np.log(np.sum(escores))\n",
        "\n",
        "        for j in range(self.classes_num):\n",
        "          dW[:, j] += (q[j] - (j == y[i])) * x[i]\n",
        "\n",
        "      loss /= x.shape[0]\n",
        "      dW /= x.shape[0]\n",
        "\n",
        "    # regularization\n",
        "    loss += self.reg * np.sum(self.W * self.W)\n",
        "    dW += 2 * self.reg * self.W\n",
        "\n",
        "    return loss, dW\n",
        "\n",
        "  def predict(self,x):\n",
        "    # optionally you can add some preprocessing here\n",
        "    x = self.add_bias(x)\n",
        "    scores = x.dot(self.W)\n",
        "    return np.argmax(scores,axis = 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyVPgrr5xjhU"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5zVN1kHd43W"
      },
      "source": [
        "## Function for accuracy checking\n",
        "\n",
        "Don't change this code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dzhRClCsdzJw"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def validate(model,dataloader):\n",
        "  y_predicted = np.array([])\n",
        "  y_gtrue = np.array([])\n",
        "  for images, class_nums in dataloader:\n",
        "    index = model.predict(images.numpy())\n",
        "    y_predicted = np.append(y_predicted,index)\n",
        "    y_gtrue = np.append(y_gtrue,class_nums.numpy())\n",
        "  return accuracy_score(y_gtrue, y_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQr1qYUlxq7X"
      },
      "source": [
        "## Train loop\n",
        "Let's train our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "phcDEj7OdpGS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 Loss: 2.1766933835324944, Accuracy:0.216\n",
            "Epoch 1 Loss: 2.0839498734882174, Accuracy:0.256\n",
            "Epoch 2 Loss: 2.0205519360405226, Accuracy:0.281\n",
            "Epoch 3 Loss: 1.971771620924751, Accuracy:0.288\n",
            "Epoch 4 Loss: 1.9321865574370938, Accuracy:0.294\n",
            "Epoch 5 Loss: 1.8989649716212484, Accuracy:0.3\n",
            "Epoch 6 Loss: 1.8703867171692519, Accuracy:0.31\n",
            "Epoch 7 Loss: 1.8453281743807823, Accuracy:0.319\n",
            "Epoch 8 Loss: 1.8230204586348902, Accuracy:0.324\n",
            "Epoch 9 Loss: 1.8029177743520088, Accuracy:0.328\n",
            "Epoch 10 Loss: 1.784620319256783, Accuracy:0.333\n",
            "Epoch 11 Loss: 1.7678270816280812, Accuracy:0.342\n",
            "Epoch 12 Loss: 1.7523059322882644, Accuracy:0.344\n",
            "Epoch 13 Loss: 1.7378740970800801, Accuracy:0.347\n",
            "Epoch 14 Loss: 1.7243850501079678, Accuracy:0.348\n",
            "Epoch 15 Loss: 1.7117194873980988, Accuracy:0.349\n",
            "Epoch 16 Loss: 1.699778959914225, Accuracy:0.349\n",
            "Epoch 17 Loss: 1.6884812815115016, Accuracy:0.352\n",
            "Epoch 18 Loss: 1.6777571485058393, Accuracy:0.356\n",
            "Epoch 19 Loss: 1.6675476040848425, Accuracy:0.363\n",
            "Epoch 20 Loss: 1.6578021037060748, Accuracy:0.366\n",
            "Epoch 21 Loss: 1.6484770160858357, Accuracy:0.366\n",
            "Epoch 22 Loss: 1.639534445445834, Accuracy:0.365\n",
            "Epoch 23 Loss: 1.6309412945467838, Accuracy:0.365\n",
            "Epoch 24 Loss: 1.6226685108972578, Accuracy:0.364\n",
            "Best accuracy is 0.366\n"
          ]
        }
      ],
      "source": [
        "from math import exp\n",
        "\n",
        "model = LinearClassifier(dataset.classes, cross_entropy_loss=True, reg=0.01)\n",
        "best_accuracy = 0\n",
        "for epoch in range(25):\n",
        "  for images, class_nums in train_loader:\n",
        "    loss = model.train(images.numpy(), class_nums.numpy(), learning_rate=1e-7)\n",
        "  accuracy = validate(model,val_loader)\n",
        "  if best_accuracy < accuracy:\n",
        "    best_accuracy = accuracy\n",
        "  print(f\"Epoch {epoch} Loss: {loss}, Accuracy:{accuracy}\")\n",
        "\n",
        "print(f\"Best accuracy is {best_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4tIFR5bwZFi"
      },
      "source": [
        "# Check model on test dataset\n",
        "\n",
        "You must get accuracy above 0.35\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "MM0pWYJlwibm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Accuracy on test:0.3515\n"
          ]
        }
      ],
      "source": [
        "test_dataset = datasets.CIFAR10(\"content\",\n",
        "                           train=False,\n",
        "                           transform = transform, # Transforms stay the same\n",
        "                           download=True)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size = batch_size)\n",
        "\n",
        "accuracy = validate(model,test_loader)\n",
        "print(f\"Accuracy on test:{accuracy}\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsImxpggG8bH"
      },
      "source": [
        "# Place for brief conclusion\n",
        "Feel free to describe troubles here.\n",
        "\n",
        "\n",
        "$lr=\\cfrac{1\\text{e-}6}{e^{\\frac{epoch}{4}}}$\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ13OmfCEb1w"
      },
      "source": [
        "# Ideas for extra work\n",
        "\n",
        "- Implenment CrossEntropyLoss function (done)\n",
        "- Implement bias trick (done)\n",
        "- Add regularization to SVM loss (done)\n",
        "- Find best learning rate and regularization strength using Cross-Validation\n",
        "- Normalize data\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
