{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWQ--FBudSXO"
      },
      "source": [
        "# 1. Train the CNN based classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6dWwjPmrePT"
      },
      "source": [
        "## Load the dataset\n",
        "Don't change this code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XKI5IOhpiYy0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess the data. Don't change this code\n",
        "from torchvision import models, datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import pickle\n",
        "\n",
        "\n",
        "# CIFAR10 z-normalization const https://github.com/facebookarchive/fb.resnet.torch/issues/180\n",
        "cifar10_mean = (0.491, 0.482, 0.447)\n",
        "cifar10_std = (0.247, 0.244, 0.262)\n",
        "\n",
        "# Data preprocessing\n",
        "transform=transforms.Compose([\n",
        "                              transforms.ToTensor(), # PIL Image to Pytorch tensor\n",
        "                              transforms.Normalize(cifar10_mean, cifar10_std) # https://pytorch.org/docs/stable/torchvision/transforms.html?highlight=transforms%20normalize#torchvision.transforms.Normalize\n",
        "                              ])\n",
        "\n",
        "dataset = datasets.CIFAR10(\"content\", train=True, transform = transform ,  download=True)\n",
        "\n",
        "# Load class names\n",
        "with open(\"content/cifar-10-batches-py/batches.meta\",'rb') as infile:\n",
        "  cifar_meta = pickle.load(infile)\n",
        "labels = cifar_meta['label_names']\n",
        "\n",
        "# Split dataset into train and val\n",
        "train_ds, val_ds, _= random_split(dataset, [10000, 2000 ,38000])\n",
        "batch_size = 256\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_ds, batch_size = batch_size)\n",
        "val_loader = DataLoader(val_ds, batch_size = batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5zVN1kHd43W"
      },
      "source": [
        "## Function for accuracy checking\n",
        "\n",
        "Don't change this code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OrxQRvfbahxH"
      },
      "outputs": [],
      "source": [
        "def validate(model,testloader, device = \"cpu\"):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        outputs = model(images.to(device))\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels.to(device)).sum().item()\n",
        "\n",
        "  return correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX-f8_6HrngE"
      },
      "source": [
        "## Implement CNN class for CIFAR10\n",
        "\n",
        "**In constructor**\n",
        "\n",
        "Define 2 - 3 convolutional layers\n",
        "\n",
        " https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "\n",
        "with corresponding in/out dimensions W_out = 1 + ((W_in - F + 2*P) / S)\n",
        "\n",
        "\n",
        "Also define max pooling : https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html\n",
        "\n",
        "and fully connected layers: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear\n",
        "\n",
        "\n",
        "**In forward**\n",
        "\n",
        "Write code for forward pass.\n",
        "Remember that first dimension is the batch dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3KX_n4_X0u8L"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;31mInit signature:\u001b[0m\n",
            "\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0min_channels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mout_channels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mkernel_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mstride\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mdilation\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mgroups\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mbias\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mpadding_mode\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'zeros'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
            "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mDocstring:\u001b[0m     \n",
            "Applies a 2D convolution over an input signal composed of several input\n",
            "planes.\n",
            "\n",
            "In the simplest case, the output value of the layer with input size\n",
            ":math:`(N, C_{\\text{in}}, H, W)` and output :math:`(N, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})`\n",
            "can be precisely described as:\n",
            "\n",
            ".. math::\n",
            "    \\text{out}(N_i, C_{\\text{out}_j}) = \\text{bias}(C_{\\text{out}_j}) +\n",
            "    \\sum_{k = 0}^{C_{\\text{in}} - 1} \\text{weight}(C_{\\text{out}_j}, k) \\star \\text{input}(N_i, k)\n",
            "\n",
            "\n",
            "where :math:`\\star` is the valid 2D `cross-correlation`_ operator,\n",
            ":math:`N` is a batch size, :math:`C` denotes a number of channels,\n",
            ":math:`H` is a height of input planes in pixels, and :math:`W` is\n",
            "width in pixels.\n",
            "\n",
            "\n",
            "This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
            "\n",
            "On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
            "\n",
            "* :attr:`stride` controls the stride for the cross-correlation, a single\n",
            "  number or a tuple.\n",
            "\n",
            "* :attr:`padding` controls the amount of padding applied to the input. It\n",
            "  can be either a string {'valid', 'same'} or an int / a tuple of ints giving the\n",
            "  amount of implicit padding applied on both sides.\n",
            "\n",
            "* :attr:`dilation` controls the spacing between the kernel points; also\n",
            "  known as the Ã  trous algorithm. It is harder to describe, but this `link`_\n",
            "  has a nice visualization of what :attr:`dilation` does.\n",
            "\n",
            "* :attr:`groups` controls the connections between inputs and outputs.\n",
            "  :attr:`in_channels` and :attr:`out_channels` must both be divisible by\n",
            "  :attr:`groups`. For example,\n",
            "\n",
            "    * At groups=1, all inputs are convolved to all outputs.\n",
            "    * At groups=2, the operation becomes equivalent to having two conv\n",
            "      layers side by side, each seeing half the input channels\n",
            "      and producing half the output channels, and both subsequently\n",
            "      concatenated.\n",
            "    * At groups= :attr:`in_channels`, each input channel is convolved with\n",
            "      its own set of filters (of size\n",
            "      :math:`\\frac{\\text{out\\_channels}}{\\text{in\\_channels}}`).\n",
            "\n",
            "The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:\n",
            "\n",
            "    - a single ``int`` -- in which case the same value is used for the height and width dimension\n",
            "    - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,\n",
            "      and the second `int` for the width dimension\n",
            "\n",
            "Note:\n",
            "    When `groups == in_channels` and `out_channels == K * in_channels`,\n",
            "    where `K` is a positive integer, this operation is also known as a \"depthwise convolution\".\n",
            "\n",
            "    In other words, for an input of size :math:`(N, C_{in}, L_{in})`,\n",
            "    a depthwise convolution with a depthwise multiplier `K` can be performed with the arguments\n",
            "    :math:`(C_\\text{in}=C_\\text{in}, C_\\text{out}=C_\\text{in} \\times \\text{K}, ..., \\text{groups}=C_\\text{in})`.\n",
            "\n",
            "Note:\n",
            "    In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting ``torch.backends.cudnn.deterministic = True``. See :doc:`/notes/randomness` for more information.\n",
            "\n",
            "Note:\n",
            "    ``padding='valid'`` is the same as no padding. ``padding='same'`` pads\n",
            "    the input so the output has the shape as the input. However, this mode\n",
            "    doesn't support any stride values other than 1.\n",
            "\n",
            "Note:\n",
            "    This module supports complex data types i.e. ``complex32, complex64, complex128``.\n",
            "\n",
            "Args:\n",
            "    in_channels (int): Number of channels in the input image\n",
            "    out_channels (int): Number of channels produced by the convolution\n",
            "    kernel_size (int or tuple): Size of the convolving kernel\n",
            "    stride (int or tuple, optional): Stride of the convolution. Default: 1\n",
            "    padding (int, tuple or str, optional): Padding added to all four sides of\n",
            "        the input. Default: 0\n",
            "    padding_mode (str, optional): ``'zeros'``, ``'reflect'``,\n",
            "        ``'replicate'`` or ``'circular'``. Default: ``'zeros'``\n",
            "    dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\n",
            "    groups (int, optional): Number of blocked connections from input\n",
            "        channels to output channels. Default: 1\n",
            "    bias (bool, optional): If ``True``, adds a learnable bias to the\n",
            "        output. Default: ``True``\n",
            "\n",
            "\n",
            "Shape:\n",
            "    - Input: :math:`(N, C_{in}, H_{in}, W_{in})` or :math:`(C_{in}, H_{in}, W_{in})`\n",
            "    - Output: :math:`(N, C_{out}, H_{out}, W_{out})` or :math:`(C_{out}, H_{out}, W_{out})`, where\n",
            "\n",
            "      .. math::\n",
            "          H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding}[0] - \\text{dilation}[0]\n",
            "                    \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor\n",
            "\n",
            "      .. math::\n",
            "          W_{out} = \\left\\lfloor\\frac{W_{in}  + 2 \\times \\text{padding}[1] - \\text{dilation}[1]\n",
            "                    \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor\n",
            "\n",
            "Attributes:\n",
            "    weight (Tensor): the learnable weights of the module of shape\n",
            "        :math:`(\\text{out\\_channels}, \\frac{\\text{in\\_channels}}{\\text{groups}},`\n",
            "        :math:`\\text{kernel\\_size[0]}, \\text{kernel\\_size[1]})`.\n",
            "        The values of these weights are sampled from\n",
            "        :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
            "        :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n",
            "    bias (Tensor):   the learnable bias of the module of shape\n",
            "        (out_channels). If :attr:`bias` is ``True``,\n",
            "        then the values of these weights are\n",
            "        sampled from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
            "        :math:`k = \\frac{groups}{C_\\text{in} * \\prod_{i=0}^{1}\\text{kernel\\_size}[i]}`\n",
            "\n",
            "Examples:\n",
            "\n",
            "    >>> # With square kernels and equal stride\n",
            "    >>> m = nn.Conv2d(16, 33, 3, stride=2)\n",
            "    >>> # non-square kernels and unequal stride and with padding\n",
            "    >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\n",
            "    >>> # non-square kernels and unequal stride and with padding and dilation\n",
            "    >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n",
            "    >>> input = torch.randn(20, 16, 50, 100)\n",
            "    >>> output = m(input)\n",
            "\n",
            ".. _cross-correlation:\n",
            "    https://en.wikipedia.org/wiki/Cross-correlation\n",
            "\n",
            ".. _link:\n",
            "    https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
            "\u001b[1;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
            "\u001b[1;31mFile:\u001b[0m           d:\\Ð²ÑÑ\\4\\Ð½Ð¸Ñ ÐºÐ¾Ð¼Ð¿ Ð·ÑÐµÐ½Ð¸Ðµ\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py\n",
            "\u001b[1;31mType:\u001b[0m           type\n",
            "\u001b[1;31mSubclasses:\u001b[0m     LazyConv2d, Conv2d, ConvBn2d, Conv2d"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "nn.Conv2d?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_zLprG7kk-Q9"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TwoLayerCNN(nn.Module):\n",
        "\n",
        "    def __init__(self, c1_out, c2_out, l1_out, class_nums=10):\n",
        "        super().__init__()\n",
        "\n",
        "        # Put your code here ...\n",
        "        self.conv_seq = nn.Sequential(\n",
        "            nn.Conv2d(3, c1_out, 3, padding=1),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(c1_out, c2_out, 3, padding=1),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(c2_out*8*8, l1_out),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(l1_out, class_nums)\n",
        "        )\n",
        "\n",
        "    def forward(self, batch):\n",
        "\n",
        "        # Put your code here ...\n",
        "        scores = self.conv_seq(batch)\n",
        "        return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3DAyLmoryLp"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl3wwvZXsgfq"
      },
      "source": [
        "## First Activate tensorboard extension\n",
        "\n",
        "Use summaryWriter to create logs: https://pytorch.org/docs/stable/tensorboard.html?highlight=summarywriter#torch.utils.tensorboard.writer.SummaryWriter\n",
        "\n",
        "Display loss and accuracy charts.\n",
        "\n",
        "You can cange log dir name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vIhmoh_Fsfsa"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9HPW9T2tSRp"
      },
      "source": [
        "### Implement training loop\n",
        "\n",
        "- Create optimizer,\n",
        "- Save loss and accuracy values into tensorboard log\n",
        "- Use GPU to speedup training process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "qcJFI3hTOJlD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "Epoch 0 Loss 1.567291617 Accuracy 0.41\n",
            "Epoch 1 Loss 0.952276886 Accuracy 0.50\n",
            "Epoch 2 Loss 0.561542571 Accuracy 0.53\n",
            "Epoch 3 Loss 0.365620434 Accuracy 0.56\n",
            "Epoch 4 Loss 0.234836191 Accuracy 0.56\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32md:\\ÐÐ¨Ð­\\4\\Ð½Ð¸Ñ ÐºÐ¾Ð¼Ð¿ Ð·ÑÐµÐ½Ð¸Ðµ\\Ex4_CNN.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%D0%92%D0%A8%D0%AD/4/%D0%BD%D0%B8%D1%81%20%D0%BA%D0%BE%D0%BC%D0%BF%20%D0%B7%D1%80%D0%B5%D0%BD%D0%B8%D0%B5/Ex4_CNN.ipynb#X16sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m output \u001b[39m=\u001b[39m model(img_batch\u001b[39m.\u001b[39mto(device))\u001b[39m.\u001b[39mcpu()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%D0%92%D0%A8%D0%AD/4/%D0%BD%D0%B8%D1%81%20%D0%BA%D0%BE%D0%BC%D0%BF%20%D0%B7%D1%80%D0%B5%D0%BD%D0%B8%D0%B5/Ex4_CNN.ipynb#X16sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, labels_batch)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%D0%92%D0%A8%D0%AD/4/%D0%BD%D0%B8%D1%81%20%D0%BA%D0%BE%D0%BC%D0%BF%20%D0%B7%D1%80%D0%B5%D0%BD%D0%B8%D0%B5/Ex4_CNN.ipynb#X16sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%D0%92%D0%A8%D0%AD/4/%D0%BD%D0%B8%D1%81%20%D0%BA%D0%BE%D0%BC%D0%BF%20%D0%B7%D1%80%D0%B5%D0%BD%D0%B8%D0%B5/Ex4_CNN.ipynb#X16sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%D0%92%D0%A8%D0%AD/4/%D0%BD%D0%B8%D1%81%20%D0%BA%D0%BE%D0%BC%D0%BF%20%D0%B7%D1%80%D0%B5%D0%BD%D0%B8%D0%B5/Ex4_CNN.ipynb#X16sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
            "File \u001b[1;32md:\\ÐÐ¨Ð­\\4\\Ð½Ð¸Ñ ÐºÐ¾Ð¼Ð¿ Ð·ÑÐµÐ½Ð¸Ðµ\\venv\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
            "File \u001b[1;32md:\\ÐÐ¨Ð­\\4\\Ð½Ð¸Ñ ÐºÐ¾Ð¼Ð¿ Ð·ÑÐµÐ½Ð¸Ðµ\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import os\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "model = TwoLayerCNN(c1_out=40, c2_out=200, l1_out=800, class_nums=10)\n",
        "\"\"\"\n",
        "  Send model to GPU\n",
        "\"\"\"\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\"\"\"\n",
        "  Setup optimizer for your model\n",
        "\"\"\"\n",
        "lr = 1e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, betas=[0.9, 0.999])\n",
        "\n",
        "lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.99 ** epoch)\n",
        "\n",
        "for epoch in range(30):\n",
        "  model.train()\n",
        "  for step, (img_batch, labels_batch) in enumerate(train_loader):\n",
        "    # if step == 2:\n",
        "    #   break\n",
        "    output = model(img_batch.to(device)).cpu()\n",
        "    loss = criterion(output, labels_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  lr_scheduler.step()\n",
        "  model.eval()\n",
        "  accuracy = validate(model,val_loader, device)\n",
        "  \"\"\"\n",
        "    Write data to tensorboard logs\n",
        "  \"\"\"\n",
        "  writer.add_scalar('loss', loss.item(), epoch)\n",
        "  writer.add_scalar('accuracy', accuracy, epoch)\n",
        "\n",
        "  # You can remove this line when the Tensorboard starts working\n",
        "  print(\"Epoch {} Loss {:.9f} Accuracy {:.2f}\".format(epoch,loss.item(),accuracy))\n",
        "\n",
        "writer.flush()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4tIFR5bwZFi"
      },
      "source": [
        "### Validat results on test dataset\n",
        "\n",
        "You must get accuracy above 0.65"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MM0pWYJlwibm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Accuracy on test:0.6352\n"
          ]
        }
      ],
      "source": [
        "test_dataset = datasets.CIFAR10(\"content\",\n",
        "                           train=False,\n",
        "                           transform = dataset.transform, # Transforms stay the same\n",
        "                           download=True)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size = batch_size)\n",
        "\n",
        "accuracy = validate(model,test_loader,device)\n",
        "print(f\"Accuracy on test:{accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbNX2ihzdQIK"
      },
      "source": [
        "# 2. Compare different Normalization methods\n",
        "\n",
        "* Add extra conv layer to your model (3-7)\n",
        "* Take three different normalization layers: BatchNorm, InstanceNorm, LayerNorm\n",
        "* Train the model with each of them.\n",
        "* Plot the loss curve for different normalization in same axis\n",
        "\n",
        "\n",
        "*Because this task is time consuming it is recommended to perform calculations on a small piece of datastat*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygt7GiHhllsk"
      },
      "outputs": [],
      "source": [
        "# Put your code here ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIvyaeSVsIl0"
      },
      "source": [
        "# Place for brief conclusion:\n",
        "\n",
        "....\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYMD7UT5BlAS"
      },
      "source": [
        "# Ideas for extra work\n",
        "\n",
        "---\n",
        "1. Evaluate the impact of the number and size of filters in convolutional layers on the accuracy.\n",
        "\n",
        "2. Evaluate the impact of the convolutional layers count on the accuracy.\n",
        "\n",
        "3. Visualize something: filters, activations ...\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
